{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17fec8af-86fc-46d3-b7ed-0bcbd083d700",
   "metadata": {},
   "source": [
    "# Managing Big Data for Connected Devices\n",
    "\n",
    "## 420-N63-NA\n",
    "\n",
    "## Kawser Wazed Nafi\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Additional RDD Operation\n",
    "At first, create the sparksession and rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13fe9fd8-0d54-46e3-a9c1-a1bc2d9d8437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ss = SparkSession.builder.master(\"local[4]\").appName(\"movielens-join\").getOrCreate();\n",
    "sc = ss.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a350f9c-cb1a-4692-9f64-429317044262",
   "metadata": {},
   "source": [
    "### Load the Ratings Data and remove the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3523f70a-2c63-4684-a731-99a533b1f78f",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o29.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/c:/Users/hiche/Desktop/Spark/Assignment 05/ratings.csv\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.io.IOException: Input path does not exist: file:/c:/Users/hiche/Desktop/Spark/Assignment 05/ratings.csv\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# sc.textFile('movies.csv') reads the movies.csv file as a text file into an RDD.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Each line of the file becomes a separate element in the RDD.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m ratingsRDDWithHeader \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mtextFile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mratings.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m header \u001b[38;5;241m=\u001b[39m \u001b[43mratingsRDDWithHeader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(header)\n\u001b[0;32m      6\u001b[0m ratingsRDD \u001b[38;5;241m=\u001b[39m ratingsRDDWithHeader\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;241m!=\u001b[39m header)\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\pyspark\\rdd.py:2888\u001b[0m, in \u001b[0;36mRDD.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2862\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   2863\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2864\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[0;32m   2865\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2886\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[0;32m   2887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2888\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2889\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[0;32m   2890\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\pyspark\\rdd.py:2822\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   2781\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2782\u001b[0m \u001b[38;5;124;03mTake the first num elements of the RDD.\u001b[39;00m\n\u001b[0;32m   2783\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2819\u001b[0m \u001b[38;5;124;03m[91, 92, 93]\u001b[39;00m\n\u001b[0;32m   2820\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2821\u001b[0m items: List[T] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 2822\u001b[0m totalParts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetNumPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2823\u001b[0m partsScanned \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   2825\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(items) \u001b[38;5;241m<\u001b[39m num \u001b[38;5;129;01mand\u001b[39;00m partsScanned \u001b[38;5;241m<\u001b[39m totalParts:\n\u001b[0;32m   2826\u001b[0m     \u001b[38;5;66;03m# The number of partitions to try in this iteration.\u001b[39;00m\n\u001b[0;32m   2827\u001b[0m     \u001b[38;5;66;03m# It is ok for this number to be greater than totalParts because\u001b[39;00m\n\u001b[0;32m   2828\u001b[0m     \u001b[38;5;66;03m# we actually cap it at totalParts in runJob.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\pyspark\\rdd.py:952\u001b[0m, in \u001b[0;36mRDD.getNumPartitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgetNumPartitions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    936\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    937\u001b[0m \u001b[38;5;124;03m    Returns the number of partitions in RDD\u001b[39;00m\n\u001b[0;32m    938\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;124;03m    2\u001b[39;00m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 952\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o29.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/c:/Users/hiche/Desktop/Spark/Assignment 05/ratings.csv\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.io.IOException: Input path does not exist: file:/c:/Users/hiche/Desktop/Spark/Assignment 05/ratings.csv\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "# sc.textFile('movies.csv') reads the movies.csv file as a text file into an RDD.\n",
    "# Each line of the file becomes a separate element in the RDD.\n",
    "ratingsRDDWithHeader = sc.textFile('ratings.csv')\n",
    "header = ratingsRDDWithHeader.first()\n",
    "print(header)\n",
    "ratingsRDD = ratingsRDDWithHeader.filter(lambda x: x != header)\n",
    "ratingsRDD = ratingsRDD.map(lambda x: x.split(','))\n",
    "ratingsRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a59686f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hellso\n"
     ]
    }
   ],
   "source": [
    "print(\"hellso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab17250-6d49-4f05-84f4-b91716211fe3",
   "metadata": {},
   "source": [
    "### Load the Movies data and remove the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e88af6bd-1563-4f3a-b49f-4dcd34d282dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m moviesRDDWithHeader \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241m.\u001b[39mtextFile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovies.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m  \u001b[38;5;66;03m# Returns the first element in the RDD, \u001b[39;00m\n\u001b[0;32m      3\u001b[0m  \u001b[38;5;66;03m# which is the first line of the file (typically the header).\u001b[39;00m\n\u001b[0;32m      4\u001b[0m moviesRDDWithHeader\u001b[38;5;241m.\u001b[39mfirst()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "moviesRDDWithHeader = sc.textFile('movies.csv')\n",
    " # Returns the first element in the RDD, \n",
    " # which is the first line of the file (typically the header).\n",
    "moviesRDDWithHeader.first()\n",
    "headerMovies = moviesRDDWithHeader.first()\n",
    "print(headerMovies)\n",
    "moviesRDD = moviesRDDWithHeader.filter(lambda x: x != headerMovies)\n",
    "moviesRDD = moviesRDD.map(lambda x: x.split(','))\n",
    "# Returns the first n elements from the RDD as a list.\n",
    "# A number (5 in this case), representing how many records to retrieve.\n",
    "moviesRDD.take(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794836ad-2b2a-4629-b2fd-3f12baf95e36",
   "metadata": {},
   "source": [
    "### Load the tags data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4590cc61-5dfa-4e68-a42c-5554ff7534b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId,movieId,tag,timestamp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['2', '60756', 'funny', '1445714994'],\n",
       " ['2', '60756', 'Highly quotable', '1445714996'],\n",
       " ['2', '60756', 'will ferrell', '1445714992'],\n",
       " ['2', '89774', 'Boxing story', '1445715207'],\n",
       " ['2', '89774', 'MMA', '1445715200']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagsRDDWithHeader = sc.textFile('tags.csv')\n",
    "tagsRDDWithHeader.first()\n",
    "headerTags = tagsRDDWithHeader.first()\n",
    "print(headerTags)\n",
    "tagsRDD = tagsRDDWithHeader.filter(lambda x: x != headerTags)\n",
    "# The split(',') function does not add commas. \n",
    "# Instead, it splits a string into a list of elements \n",
    "# wherever a comma exists in the original string.\n",
    "tagsRDD = tagsRDD.map(lambda x: x.split(','))\n",
    "tagsRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30600a9f-713c-433e-9b0b-2dd919218db4",
   "metadata": {},
   "source": [
    "### Filterout only the Movies with Funny Tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5be3564f-9e7c-4444-8792-8c4ca7fe048b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2', '60756', 'funny', '1445714994'],\n",
       " ['62', '2953', 'funny', '1525636885'],\n",
       " ['62', '3114', 'funny', '1525636913'],\n",
       " ['62', '60756', 'funny', '1528934381'],\n",
       " ['62', '68848', 'funny', '1527274322']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x represents one row of the dataset.\n",
    "# Takes the RDD (without its header), checks 'rows' at index 2, at 'column' tag, where the entries are funny, case insensitive.\n",
    "filteredFunnyTagsRDD = tagsRDD.filter(lambda x: 'funny' in x[2].lower())\n",
    "filteredFunnyTagsRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f9779d-4864-475f-a387-762ddab22b10",
   "metadata": {},
   "source": [
    "### Multivalue Mapping Creation\n",
    "Till now we have created a number of RDDs. At this point, our goal is to create a multicolumn/multivalue RDD mapping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e95c2e-d313-4973-b2c3-f57e736e6bfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filteredFunnyTagsRDD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m moviesWithTagFunnyRDD \u001b[38;5;241m=\u001b[39m \u001b[43mfilteredFunnyTagsRDD\u001b[49m\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x[\u001b[38;5;241m1\u001b[39m], (x[\u001b[38;5;241m0\u001b[39m], x[\u001b[38;5;241m2\u001b[39m])))\u001b[38;5;241m.\u001b[39mreduceByKey(\u001b[38;5;28;01mlambda\u001b[39;00m x,y: (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunny\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      2\u001b[0m moviesWithTagFunnyRDD\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filteredFunnyTagsRDD' is not defined"
     ]
    }
   ],
   "source": [
    "moviesWithTagFunnyRDD = filteredFunnyTagsRDD.map(lambda x: (x[1], (x[0], x[2]))).reduceByKey(lambda x,y: ('1', 'funny')).map(lambda x: x[0]).map(lambda x: (x,1))\n",
    "moviesWithTagFunnyRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72308db2-07e8-4c33-85a3-f3f332d221c0",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "It's a big line performing a big value operation. Could you please divide the operation in small operations and check what is happening with every small operation? Please list your findings one by one and explain in your own word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d94141",
   "metadata": {},
   "source": [
    "<h1>ANSWER TO EXERCISE 1</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437e13d4",
   "metadata": {},
   "source": [
    "<p>In this exercise, we're joining two RDDs to find the highest average-rated movie under the \"funny\" genre.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1dda00",
   "metadata": {},
   "source": [
    "<p>The map() function converts each movie into a key-value pair.<br>\n",
    "    The first index being the movieId and the second and third indexes forming<br>\n",
    "    its values. movieId (key) (title, genre) (values)\n",
    "</p>\n",
    "<p>Each record represents a movie.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45db7ca2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'moviesRDD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m moviesRDDwithKey \u001b[38;5;241m=\u001b[39m \u001b[43mmoviesRDD\u001b[49m\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x[\u001b[38;5;241m0\u001b[39m], (x[\u001b[38;5;241m1\u001b[39m], x[\u001b[38;5;241m2\u001b[39m])))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'moviesRDD' is not defined"
     ]
    }
   ],
   "source": [
    "moviesRDDwithKey = moviesRDD.map(lambda x: (x[0], (x[1], x[2])))\n",
    "moviesWithTagFunnyRDD = (\n",
    "    filteredFunnyTagsRDD\n",
    "    # .map(lambda x: (x[1], (x[0], x[2])))\n",
    "    .reduceByKey(lambda x, y: ('1', 'funny'))\n",
    "    .map(lambda x: x[0])\n",
    "    .map(lambda x: (x, 1))\n",
    ")\n",
    "moviesWithTagFunnyRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4845f2e",
   "metadata": {},
   "source": [
    "<p></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0e4064",
   "metadata": {},
   "source": [
    "<p>The reduceByKey() here combines all the values with the same key with movieId. It then replace all combined values with ('1', 'funny').</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24052ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesWithTagFunnyRDD = (\n",
    "    filteredFunnyTagsRDD\n",
    "    .map(lambda x: (x[1], (x[0], x[2])))\n",
    "    #.reduceByKey(lambda x, y: ('1', 'funny'))\n",
    "    .map(lambda x: x[0])\n",
    "    .map(lambda x: (x, 1))\n",
    ")\n",
    "moviesWithTagFunnyRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6391bb2d",
   "metadata": {},
   "source": [
    "<p>The .map(lamda x: x[0]), takes the movieId from the key-value pairs. The movieId being at index 0.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaffe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesWithTagFunnyRDD = (\n",
    "    filteredFunnyTagsRDD\n",
    "    .map(lambda x: (x[1], (x[0], x[2])))\n",
    "    .reduceByKey(lambda x, y: ('1', 'funny'))\n",
    "    #.map(lambda x: x[0])\n",
    "    .map(lambda x: (x, 1))\n",
    ")\n",
    "moviesWithTagFunnyRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee9b575",
   "metadata": {},
   "source": [
    "<p>The .map(lambda x: (x, 1)) converts each movieId into a pair of (movieId, 1). I guess that could be used for a .reduceByKey() operation for counting the number of funny tagged movies.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0268a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesWithTagFunnyRDD = (\n",
    "    filteredFunnyTagsRDD\n",
    "    .map(lambda x: (x[1], (x[0], x[2])))\n",
    "    .reduceByKey(lambda x, y: ('1', 'funny'))\n",
    "    .map(lambda x: x[0])\n",
    "    # .map(lambda x: (x, 1))\n",
    ")\n",
    "moviesWithTagFunnyRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9576b77",
   "metadata": {},
   "source": [
    "<p>The moviesWithTagFunnyRDD.take(5) prints the first 5 records. The take() is like the first() but you can specify the number of the first few records by passing a value to it. Therefore, passing 5, means that you want the 5 first lines.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bc7ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesWithTagFunnyRDD = (\n",
    "    filteredFunnyTagsRDD\n",
    "    .map(lambda x: (x[1], (x[0], x[2])))\n",
    "    .reduceByKey(lambda x, y: ('1', 'funny'))\n",
    "    .map(lambda x: x[0])\n",
    "    .map(lambda x: (x, 1))\n",
    ")\n",
    "# moviesWithTagFunnyRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9d130a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bc7fa6e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbf3ae1b-07b2-4735-800b-a533a834f0b2",
   "metadata": {},
   "source": [
    "### RDDs Join Operation\n",
    "We are recurring the problem we discussed in our previous exercise. We are going to find out the highest average rating movie from the movie list. This time, we are going to see only the highest average rated movie under \"funny\" genere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a0d552a-6700-479d-90b1-68f124e4ad52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('60756', (1, ('Step Brothers (2008)', 'Comedy'))),\n",
       " ('2953', (1, ('Home Alone 2: Lost in New York (1992)', 'Children|Comedy'))),\n",
       " ('71535', (1, ('Zombieland (2009)', 'Action|Comedy|Horror'))),\n",
       " ('88405', (1, ('Friends with Benefits (2011)', 'Comedy|Romance'))),\n",
       " ('99114', (1, ('Django Unchained (2012)', 'Action|Drama|Western')))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moviesRDDwithKey = moviesRDD.map(lambda x: (x[0], (x[1], x[2])))\n",
    "moviesRDDwithKey.take(5)\n",
    "\n",
    "moviesWithTagFunnyRDD.join(moviesRDDwithKey).take(5)\n",
    "#Tag Indicator (1) → Means this movie was tagged as \"funny\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfe265a-812c-4c23-ad91-94078bc484cb",
   "metadata": {},
   "source": [
    "From the Data we can see that we have performed a new tuple configuration, where MovieID is merged with Movie name and the Funny Genere. We should see how the data tuple is configured and distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a53dd97",
   "metadata": {},
   "source": [
    "<h1>EXPLANATION OF THE NEW CONFIGURATION</h1>\n",
    "<p>x[0] = '60756' → Movie ID.</p>\n",
    "<p>x[1] = (1, ('Step Brothers (2008)', 'Comedy')) → A tuple containing:<br>\n",
    "1 (marker indicating the movie has been tagged as funny)<br>\n",
    "('Step Brothers (2008)', 'Comedy') (tuple containing movie title and genre)</br>\n",
    "<p>How to access specific elements inside this tuple?</p>\n",
    "<p>x[1][0] = 1<br>\n",
    "x[1][1] = ('Step Brothers (2008)', 'Comedy')\n",
    "</p>\n",
    "<h1>***************************</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ec0fc8-7c8b-4558-bd81-e80de2b980ea",
   "metadata": {},
   "source": [
    "('60756', (1, ('Step Brothers (2008)', 'Comedy')))\n",
    "\n",
    "x[0] = '60756'\n",
    "x[1] = (1, ('Step Brothers (2008)', 'Comedy'))\n",
    "\n",
    "x[1][0] = 1\n",
    "x[1][1] = ('Step Brothers (2008)', 'Comedy')\n",
    "\n",
    "x[1][1][0] = 'Step Brothers (2008)'\n",
    "x[1][1][1] = 'Comedy'\n",
    "\n",
    "map(lambda x: (x[0], x[1][1][0], x[1][1][1]))\n",
    "\n",
    "\n",
    "(x[0],) + x[1][1]\n",
    "\n",
    "Based on this analysis, we now need to perform the mapping in such a way that movieID will represent Movie Name and Genere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141872d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before applying .map(), each element in the RDD looks like this:\n",
    "\n",
    "# ('60756', (1, ('Step Brothers (2008)', 'Comedy')))\n",
    "\n",
    "# Breaking it down:\n",
    "\n",
    "# x[0] → '60756' (Movie ID)\n",
    "# x[1] → (1, ('Step Brothers (2008)', 'Comedy'))\n",
    "# x[1][0] → 1 (funny tag marker)\n",
    "# x[1][1] → ('Step Brothers (2008)', 'Comedy') (Movie Name & Genre)\n",
    "\n",
    "# The comma (,) makes sure that x[0] (the Movie ID) is treated as a tuple.\n",
    "# Without the comma, it would be just a string.\n",
    "\n",
    "# Example:\n",
    "\n",
    "# x[0] = '60756'\n",
    "# (x[0],)  → ('60756',)   # Now it's a tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48e3478d-d8e2-4f73-aa45-8cff0e2094b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('60756', ('Step Brothers (2008)', 'Comedy')),\n",
       " ('2953', ('Home Alone 2: Lost in New York (1992)', 'Children|Comedy')),\n",
       " ('71535', ('Zombieland (2009)', 'Action|Comedy|Horror')),\n",
       " ('88405', ('Friends with Benefits (2011)', 'Comedy|Romance')),\n",
       " ('99114', ('Django Unchained (2012)', 'Action|Drama|Western'))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moviesWithTagFunnyJoinedRDD = moviesWithTagFunnyRDD.join(moviesRDDwithKey).map(lambda x: (x[0],) + x[1][1]).map(lambda x: (x[0], (x[1],x[2])))\n",
    "moviesWithTagFunnyJoinedRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696b12b7-eaab-4c84-b8de-1d9d122129ec",
   "metadata": {},
   "source": [
    "After this, our next step is to match the movieID with the ratings and generate a representational tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f22d3900-478a-4102-9158-69657fa3bf9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', '4.0'), ('3', '4.0'), ('6', '4.0'), ('47', '5.0'), ('50', '5.0')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsKeyValueRDD = ratingsRDD.map(lambda x: (x[1], x[2]))\n",
    "ratingsKeyValueRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cf13ae-e26e-459e-8c4e-6b4c17fa99b3",
   "metadata": {},
   "source": [
    "Finally, we need to join our funny Genere movies with the ratings to find out the best average rated funny movie in the movie list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e5274b5-4c87-404a-9c18-0010a33d416e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('296', 4.197068403908795, 'Pulp Fiction (1994)')\n"
     ]
    }
   ],
   "source": [
    "ratingsFilteredWithTagFunnyRDD = ratingsKeyValueRDD.join(moviesWithTagFunnyJoinedRDD).map(lambda x: (x[0], float(x[1][0]), x[1][1][0]))\n",
    "ratingsFilteredWithTagFunnyRDD=ratingsFilteredWithTagFunnyRDD.map(lambda x: (x[0], (x[1], 1, x[2]))).reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1], x[2])).filter(lambda x: x[1][1] > 10).map(lambda x: (x[0], x[1][0]/x[1][1], x[1][2]))\n",
    "print(ratingsFilteredWithTagFunnyRDD.max(lambda x: x[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a889e9-f819-49a1-8457-1a00d509bed3",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Can we get the top 5 average rated funny movies instead of just the top? Give a try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c593066-3b7c-4ab2-bb6e-8e09137d825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratingsFilteredWithTagFunnyRDD contains:\n",
    "\n",
    "# ('60756', 4.1, 'Step Brothers (2008)')\n",
    "# ('2953', 4.8, 'Home Alone 2: Lost in New York (1992)')\n",
    "# ('71535', 4.5, 'Zombieland (2009)')\n",
    "# ('88405', 4.2, 'Friends with Benefits (2011)')\n",
    "# ('99114', 4.9, 'Django Unchained (2012)')\n",
    "\n",
    "# Format: (movieId, avg_rating, movie_title)\n",
    "\n",
    "# Sorting in Descending Order\n",
    "\n",
    "# .takeOrdered(5, key=lambda x: -x[1])\n",
    "# The negative sign (-x[1]) sorts from highest to lowest.\n",
    "\n",
    "top5FunnyMovies = ratingsFilteredWithTagFunnyRDD.takeOrdered(5, key=lambda x: -x[1])\n",
    "print(top5FunnyMovies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d612d42-755b-4ac9-a015-13b72fbae81c",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Create different RDDs with joined by key. For each RDD, create an RDD that is a tuple where the first element is the key which is the movie ID, and the second element is the rest of the data that we want. Use the ratings file and the movies file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb9496-2788-4458-b7ea-85ad38e9ce73",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load Data and Remove Headers\u001b[39;00m\n\u001b[0;32m      8\u001b[0m ratingsRDDWithHeader \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mtextFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/hiche/Desktop/Spark/Assignment 05/ratings.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m headerRatings \u001b[38;5;241m=\u001b[39m \u001b[43mratingsRDDWithHeader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Get header\u001b[39;00m\n\u001b[0;32m     10\u001b[0m ratingsRDD \u001b[38;5;241m=\u001b[39m ratingsRDDWithHeader\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m row: row \u001b[38;5;241m!=\u001b[39m headerRatings)  \u001b[38;5;66;03m# Remove header\u001b[39;00m\n\u001b[0;32m     12\u001b[0m moviesRDDWithHeader \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mtextFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/hiche/Desktop/Spark/Assignment 05/movies.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\pyspark\\rdd.py:2888\u001b[0m, in \u001b[0;36mRDD.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2862\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   2863\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2864\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[0;32m   2865\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2886\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[0;32m   2887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2888\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2889\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[0;32m   2890\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\pyspark\\rdd.py:2855\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   2852\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2854\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[1;32m-> 2855\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2857\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m   2858\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\pyspark\\context.py:2510\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   2508\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m   2509\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2510\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"MovieRatingsAnalysis\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load Data and Remove Headers\n",
    "ratingsRDDWithHeader = sc.textFile(\"C:/Users/hiche/Desktop/Spark/Assignment 05/ratings.csv\")\n",
    "headerRatings = ratingsRDDWithHeader.first()  # Get header\n",
    "ratingsRDD = ratingsRDDWithHeader.filter(lambda row: row != headerRatings)  # Remove header\n",
    "\n",
    "moviesRDDWithHeader = sc.textFile(\"C:/Users/hiche/Desktop/Spark/Assignment 05/movies.csv\")\n",
    "headerMovies = moviesRDDWithHeader.first()\n",
    "moviesRDD = moviesRDDWithHeader.filter(lambda row: row != headerMovies)\n",
    "\n",
    "# Filter Out Bad Rows\n",
    "# .filter(...) Keeps rows with exactly 3 elements (movieId, title, genre).\n",
    "moviesRDDWithKey = moviesRDD.map(lambda x: x.split(',')) \\\n",
    "                            .filter(lambda x: len(x) == 3) \\\n",
    "                            .map(lambda x: (x[0], (x[1], x[2])))\n",
    "# .filter(...) Keeps rows with at least 3 elements (userId, movieId, rating).\n",
    "ratingsRDDWithKey = ratingsRDD.map(lambda x: x.split(',')) \\\n",
    "                              .filter(lambda x: len(x) >= 3) \\\n",
    "                              .map(lambda x: (x[1], float(x[2])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7e0c00-c588-4f23-a305-c2a39f3ea960",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "Can you join two RDDs you created at exercise 3? Please join them and see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b006e-b82e-495b-aa03-1d5c0010beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Movies and Ratings RDDs on movieId\n",
    "moviesWithRatingsRDD = moviesRDDWithKey.join(ratingsRDDWithKey)\n",
    "\n",
    "# Restructure Data\n",
    "moviesWithAggregatedRatingsRDD = moviesWithRatingsRDD.map(lambda x: (x[0], (x[1][0][0], x[1][1])))\n",
    "\n",
    "# Check for Empty RDD\n",
    "if moviesWithAggregatedRatingsRDD.isEmpty():\n",
    "    print(\"⚠️ The RDD is empty. Check data processing steps.\")\n",
    "else:\n",
    "    print(moviesWithAggregatedRatingsRDD.take(5))  # Display first 5 results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac091bb-7050-400e-a24a-c15ee8bbea79",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Can you explain with the below mapping operation is performing? You should execute the following operation on your joined RDD you created in Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da12c39",
   "metadata": {},
   "source": [
    "<h1>Explanation of exercise 5</h1>\n",
    "<p>The first mapping concatenates the title and the rating. Which will result in an error because we cannot concatenate a string with a float in Python. The floats needs to be casted with str(...). <br>\n",
    "When we say concatenate, it means it combines the two, just like we usually concatenate strings.\n",
    "</p>\n",
    "<p>The second mapping converts the movieId into a tuple (using the commas x[0],)<br> \n",
    "Then, it concatenates it with the string 'Toy Story... 4.5'.</p>\n",
    "<p>The take(2), retrieves the first 2 elements of the RDD.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82e5bccf-db92-48a5-ae9c-608e21030af4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'joinedRatingsMovieRDD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mjoinedRatingsMovieRDD\u001b[49m\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x[\u001b[38;5;241m0\u001b[39m], (x[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m x[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m])))\u001b[38;5;241m.\u001b[39m\\\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: ((x[\u001b[38;5;241m0\u001b[39m],) \u001b[38;5;241m+\u001b[39m x[\u001b[38;5;241m1\u001b[39m]))\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'joinedRatingsMovieRDD' is not defined"
     ]
    }
   ],
   "source": [
    "joinedRatingsMovieRDD.map(lambda x: (x[0], (x[1][0]+ x[1][1]))).map(lambda x: ((x[0],) + x[1])).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b2e69a-d506-4540-a57e-3e68153cacb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f8d891a-6fe2-441c-854b-04229cd7c487",
   "metadata": {},
   "source": [
    "### Exercise 6 (Advanced)\n",
    "\n",
    "We have done enough works with multivalued mapreduce operations. Now is the time to use your own understanding to perform the multivalued ratings in your own style.\n",
    "\n",
    "I have given your 4 files together: Links, Movies, Ratings, Tags. You have to use at least 3 of these files. You first need to derive\n",
    "your proposal based on the example given over here(You proposal will not be related to find out the highest average rated movie. It could be anything else, such as adding links of the movies under Action genere) . Based on your analysis proposal, perform the data analysis as shown in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91160595",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o27.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/c:/Users/hiche/Desktop/Spark/Assignment 05/links.csv\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.io.IOException: Input path does not exist: file:/c:/Users/hiche/Desktop/Spark/Assignment 05/links.csv\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m tagsRDD \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mtextFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Not used in this analysis but loaded for completeness\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#  Step 3: Remove Headers\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m headerLinks \u001b[38;5;241m=\u001b[39m \u001b[43mlinksRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m headerMovies \u001b[38;5;241m=\u001b[39m moviesRDD\u001b[38;5;241m.\u001b[39mfirst()\n\u001b[0;32m     16\u001b[0m headerRatings \u001b[38;5;241m=\u001b[39m ratingsRDD\u001b[38;5;241m.\u001b[39mfirst()\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\pyspark\\rdd.py:2888\u001b[0m, in \u001b[0;36mRDD.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2862\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   2863\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2864\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[0;32m   2865\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2886\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[0;32m   2887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2888\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2889\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[0;32m   2890\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\pyspark\\rdd.py:2822\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   2781\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2782\u001b[0m \u001b[38;5;124;03mTake the first num elements of the RDD.\u001b[39;00m\n\u001b[0;32m   2783\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2819\u001b[0m \u001b[38;5;124;03m[91, 92, 93]\u001b[39;00m\n\u001b[0;32m   2820\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2821\u001b[0m items: List[T] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 2822\u001b[0m totalParts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetNumPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2823\u001b[0m partsScanned \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   2825\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(items) \u001b[38;5;241m<\u001b[39m num \u001b[38;5;129;01mand\u001b[39;00m partsScanned \u001b[38;5;241m<\u001b[39m totalParts:\n\u001b[0;32m   2826\u001b[0m     \u001b[38;5;66;03m# The number of partitions to try in this iteration.\u001b[39;00m\n\u001b[0;32m   2827\u001b[0m     \u001b[38;5;66;03m# It is ok for this number to be greater than totalParts because\u001b[39;00m\n\u001b[0;32m   2828\u001b[0m     \u001b[38;5;66;03m# we actually cap it at totalParts in runJob.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\pyspark\\rdd.py:952\u001b[0m, in \u001b[0;36mRDD.getNumPartitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgetNumPartitions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    936\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    937\u001b[0m \u001b[38;5;124;03m    Returns the number of partitions in RDD\u001b[39;00m\n\u001b[0;32m    938\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;124;03m    2\u001b[39;00m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 952\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o27.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/c:/Users/hiche/Desktop/Spark/Assignment 05/links.csv\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.io.IOException: Input path does not exist: file:/c:/Users/hiche/Desktop/Spark/Assignment 05/links.csv\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"MovieRatingsAnalysis\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Step 2: Load Data\n",
    "linksRDD = sc.textFile(\"links.csv\")\n",
    "moviesRDD = sc.textFile(\"movies.csv\")\n",
    "ratingsRDD = sc.textFile(\"ratings.csv\")\n",
    "tagsRDD = sc.textFile(\"tags.csv\")  # Not used in this analysis but loaded for completeness\n",
    "\n",
    "#  Step 3: Remove Headers\n",
    "headerLinks = linksRDD.first()\n",
    "headerMovies = moviesRDD.first()\n",
    "headerRatings = ratingsRDD.first()\n",
    "headerTags = tagsRDD.first()\n",
    "\n",
    "linksRDD = linksRDD.filter(lambda row: row != headerLinks).map(lambda x: x.split(','))\n",
    "moviesRDD = moviesRDD.filter(lambda row: row != headerMovies).map(lambda x: x.split(','))\n",
    "ratingsRDD = ratingsRDD.filter(lambda row: row != headerRatings).map(lambda x: x.split(','))\n",
    "tagsRDD = tagsRDD.filter(lambda row: row != headerTags).map(lambda x: x.split(','))  # Not used here\n",
    "\n",
    "# Step 4: Process the Data\n",
    "# Count the number of ratings per movie\n",
    "ratingsCountRDD = ratingsRDD.map(lambda x: (x[1], 1)) \\\n",
    "                             .reduceByKey(lambda x, y: x + y)  # (movieId, rating_count)\n",
    "\n",
    "# Map movies to (movieId, title)\n",
    "moviesRDDWithKey = moviesRDD.map(lambda x: (x[0], x[1]))  # (movieId, title)\n",
    "\n",
    "# Map links to (movieId, IMDb_ID)\n",
    "linksRDDWithKey = linksRDD.map(lambda x: (x[0], x[1]))  # (movieId, IMDb_ID)\n",
    "\n",
    "# Step 5: Join the Datasets\n",
    "# Join ratings with movies to get movie titles\n",
    "moviesWithRatingsRDD = ratingsCountRDD.join(moviesRDDWithKey)  # (movieId, (rating_count, title))\n",
    "\n",
    "# Join with links to get IMDb IDs\n",
    "finalRDD = moviesWithRatingsRDD.join(linksRDDWithKey)  # (movieId, ((rating_count, title), IMDb_ID))\n",
    "\n",
    "# Restructure the data to (title, rating_count, IMDb_ID)\n",
    "finalRDD = finalRDD.map(lambda x: (x[1][0][1], x[1][0][0], x[1][1]))  # (title, rating_count, IMDb_ID)\n",
    "\n",
    "# Step 6: Find the Top 5 Most Rated Movies\n",
    "# Sort by rating_count in descending order and take the top 5\n",
    "top5MostRatedMovies = finalRDD.sortBy(lambda x: x[1], ascending=False).take(5)\n",
    "\n",
    "# Step 7: Display the Results\n",
    "for movie in top5MostRatedMovies:\n",
    "    print(f\" Title: {movie[0]},  Ratings Count: {movie[1]},  IMDb ID: {movie[2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf96c4c-07fa-4931-8f7a-5a6f150dabf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55ffc35-2aec-41b3-9a9f-00292c7c8a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd61350a-b865-440f-991d-e9860871f1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
