{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af44a061-6748-459e-ad43-917286f21a98",
   "metadata": {},
   "source": [
    "# Managing Big Data for Connected Devices\n",
    "\n",
    "## 420-N63-NA\n",
    "\n",
    "## Kawser Wazed Nafi\n",
    " ----------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "## StructType & StructField\n",
    "\n",
    "PySpark StructType & StructField classes are used to programmatically specify the schema to the DataFrame and create complex columns like nested struct, array, and map columns. StructType is a collection of StructFieldâ€™s that defines column name, column data type, boolean to specify if the field can be nullable or not and metadata.\n",
    "\n",
    "If a data is given in an unstructured way, StructType and StructField are used to make them structured and use them as input the PySpark Dataframe. This helped us to perform the data analysis with proper data understanding and with more structured and regulated way.\n",
    "\n",
    "At the time of creating a PySpark Dataframe, we can specify the structure of the data using StructType and StructField.\n",
    "\n",
    "## StructType Example 1\n",
    "\n",
    "Let's us consider an input data which has no structure itself. Using StructType we can give the data a name as well as we can define the dataType of the given data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d79f951f-4706-4490-90c2-ebfb216345d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|id   |gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|James    |          |Smith   |36636|M     |3000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "ss = SparkSession.builder.master(\"local[4]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "# Explanation:\n",
    "# StructType defines the schema with six fields: firstname, middlename, lastname, id, gender, and salary.\n",
    "\n",
    "# Each field is defined using StructField, specifying:\n",
    "\n",
    "# The column name (e.g., firstname)\n",
    "# The data type (e.g., StringType(), IntegerType())\n",
    "# The nullable flag (True means the column can have null values)\n",
    "# The DataFrame is created using createDataFrame(), and the schema is explicitly applied.\n",
    "schema = StructType([ \\\n",
    "    StructField(\"firstname\",StringType(),True), \\\n",
    "    StructField(\"middlename\",StringType(),True), \\\n",
    "    StructField(\"lastname\",StringType(),True), \\\n",
    "    StructField(\"id\", StringType(), True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "    StructField(\"salary\", IntegerType(), True) \\\n",
    "  ])\n",
    " \n",
    "dataframe = ss.createDataFrame(data=data,schema=schema)\n",
    "dataframe.printSchema()\n",
    "dataframe.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e013017-2d5c-4b0a-9ec4-3ebfa911c8af",
   "metadata": {},
   "source": [
    "## StructType Example 2\n",
    "\n",
    "Let's us consider that the same input data contains the firstname, middle name and last name section as tuple. When we have an additional tuple in the given data, we can consider this data as a nested structure. To address that data in your program, you have to used nexted StrutType Object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af40096f-1cb8-4f4d-be91-9eb4f425d3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+--------------------+-----+------+------+\n",
      "|name                |id   |gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|{James, , Smith}    |36636|M     |3100  |\n",
      "|{Michael, Rose, }   |40288|M     |4300  |\n",
      "|{Robert, , Williams}|42114|M     |1400  |\n",
      "|{Maria, Anne, Jones}|39192|F     |5500  |\n",
      "|{Jen, Mary, Brown}  |     |F     |-1    |\n",
      "+--------------------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "structureData = [\n",
    "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
    "  ]\n",
    "structureSchema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('id', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', IntegerType(), True)\n",
    "         ])\n",
    "\n",
    "dataframe2 = ss.createDataFrame(data=structureData,schema=structureSchema)\n",
    "dataframe2.printSchema()\n",
    "dataframe2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e621e92-58da-491d-bf5a-c89ee303edb8",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "From our latest Movie dataset we got the following data:\n",
    "    \n",
    "data = [((1,4.2),(1,\"funny\")),\n",
    "((2,4.5),(3,\"funny\")),\n",
    "((1,4.0),(6,\"funny\")),\n",
    "((3,5.0),(47,\"action\")),\n",
    "((4,4.3),(50,\"romantic\")),\n",
    "((3,3.2),(70,\"biography\")),\n",
    "((4,5.0),(101,\"biography\")),\n",
    "((4,4.6),(110,\"Scientific\")),\n",
    "((1,5.0),(151,\"action\")),\n",
    "((1,4.6),( 157,\"action\")),\n",
    "((2,3.5),(167,\"funny\")),\n",
    "((1,4.1),(172,\"funny\")),\n",
    "((3,4.7),(181,\"action\")),\n",
    "((4,3.9),(192,\"romantic\")),\n",
    "((3,3.8),(201,\"biography\")),\n",
    "((4,5.0),(211,\"biography\")),\n",
    "((4,4.6),(224,\"Scientific\")),\n",
    "((1,5.0),(231,\"action\"))]\n",
    "\n",
    "the data is divided in the following structure: (userID, rating),(movieID, generes)\n",
    "\n",
    "Please structure the data and load it into dataframe for additional additional study.\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300cbb32-7496-449e-86f9-32cb37fa0aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
    "\n",
    "# Initialize Spark Session\n",
    "ss = SparkSession.builder.master(\"local[4]\").appName(\"MovieDataStructuring\").getOrCreate()\n",
    "\n",
    "# Provided Data\n",
    "data = [\n",
    "    ((1, 4.2), (1, \"funny\")),\n",
    "    ((2, 4.5), (3, \"funny\")),\n",
    "    ((1, 4.0), (6, \"funny\")),\n",
    "    ((3, 5.0), (47, \"action\")),\n",
    "    ((4, 4.3), (50, \"romantic\")),\n",
    "    ((3, 3.2), (70, \"biography\")),\n",
    "    ((4, 5.0), (101, \"biography\")),\n",
    "    ((4, 4.6), (110, \"Scientific\")),\n",
    "    ((1, 5.0), (151, \"action\")),\n",
    "    ((1, 4.6), (157, \"action\")),\n",
    "    ((2, 3.5), (167, \"funny\")),\n",
    "    ((1, 4.1), (172, \"funny\")),\n",
    "    ((3, 4.7), (181, \"action\")),\n",
    "    ((4, 3.9), (192, \"romantic\")),\n",
    "    ((3, 3.8), (201, \"biography\")),\n",
    "    ((4, 5.0), (211, \"biography\")),\n",
    "    ((4, 4.6), (224, \"Scientific\")),\n",
    "    ((1, 5.0), (231, \"action\"))\n",
    "]\n",
    "\n",
    "# Defining the Schema\n",
    "schema = StructType([\n",
    "    StructField('user_rating', StructType([\n",
    "        StructField('userID', IntegerType(), True),\n",
    "        StructField('rating', FloatType(), True)\n",
    "    ])),\n",
    "    StructField('movie_info', StructType([\n",
    "        StructField('movieID', IntegerType(), True),\n",
    "        StructField('genre', StringType(), True)\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Creating DataFrame\n",
    "df = ss.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Display Schema and Data\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Flattening the nested structure for easier analysis\n",
    "df_flat = df.select(\n",
    "    df.user_rating.userID.alias(\"userID\"),\n",
    "    df.user_rating.rating.alias(\"rating\"),\n",
    "    df.movie_info.movieID.alias(\"movieID\"),\n",
    "    df.movie_info.genre.alias(\"genre\")\n",
    ")\n",
    "\n",
    "# Displaying the Flattened Data\n",
    "df_flat.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ff2b4d-0e67-4b4d-b899-b9d7cbec54fc",
   "metadata": {},
   "source": [
    "## Transform\n",
    "\n",
    "Another method or API provided by Spark to prepare the dataFrame for further analysis is pyspark.sql.DataFrame.transform(). The pyspark.sql.DataFrame.transform() is used to chain the custom transformations and this function returns the new DataFrame after applying the specified transformations.\n",
    "This function returns the new data maintaining the same number of rows.\n",
    "\n",
    "### Syntax\n",
    "\n",
    "DataFrame.transform(func: Callable[[â€¦], DataFrame], *args: Any, **kwargs: Any) â†’ pyspark.sql.dataframe.DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a438c10c-7744-4995-8922-a18400155a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CourseName: string (nullable = true)\n",
      " |-- fee: long (nullable = true)\n",
      " |-- discount: long (nullable = true)\n",
      "\n",
      "+----------+----+--------+\n",
      "|CourseName|fee |discount|\n",
      "+----------+----+--------+\n",
      "|Java      |4000|5       |\n",
      "|Python    |4600|10      |\n",
      "|Scala     |4100|15      |\n",
      "|Scala     |4500|15      |\n",
      "|PHP       |3000|20      |\n",
      "+----------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "ss = SparkSession.builder \\\n",
    "            .appName('SparkByExamples.com') \\\n",
    "            .getOrCreate()\n",
    "\n",
    "# Prepare Data\n",
    "simpleData = ((\"Java\",4000,5), \\\n",
    "    (\"Python\", 4600,10),  \\\n",
    "    (\"Scala\", 4100,15),   \\\n",
    "    (\"Scala\", 4500,15),   \\\n",
    "    (\"PHP\", 3000,20),  \\\n",
    "  )\n",
    "columns= [\"CourseName\", \"fee\", \"discount\"]\n",
    "\n",
    "# Create DataFrame\n",
    "dataframe = ss.createDataFrame(data = simpleData, schema = columns)\n",
    "dataframe.printSchema()\n",
    "dataframe.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5158a7-fe5c-496c-b224-9e3676a49d1f",
   "metadata": {},
   "source": [
    "We can add custom transformation function in our program, pass our dataframe to them and will finally get the transformed data together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b35f79c-f833-4eee-bd3c-277fc8d717e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+-------+--------------+\n",
      "|CourseName| fee|discount|new_fee|discounted_fee|\n",
      "+----------+----+--------+-------+--------------+\n",
      "|      JAVA|4000|       5|   3000|        2850.0|\n",
      "|    PYTHON|4600|      10|   3600|        3240.0|\n",
      "|     SCALA|4100|      15|   3100|        2635.0|\n",
      "|     SCALA|4500|      15|   3500|        2975.0|\n",
      "|       PHP|3000|      20|   2000|        1600.0|\n",
      "+----------+----+--------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Custom transformation 1\n",
    "from pyspark.sql.functions import upper\n",
    "def to_upper_str_columns(dataframe):\n",
    "    return dataframe.withColumn(\"CourseName\",upper(dataframe.CourseName))\n",
    "\n",
    "# Custom transformation 2\n",
    "def reduce_price(dataframe,reduceBy):\n",
    "    return dataframe.withColumn(\"new_fee\",dataframe.fee - reduceBy)\n",
    "\n",
    "# Custom transformation 3\n",
    "def apply_discount(dataframe):\n",
    "    return dataframe.withColumn(\"discounted_fee\",  \\\n",
    "             dataframe.new_fee - (dataframe.new_fee * dataframe.discount) / 100)\n",
    "\n",
    "# We are going to reduce the reduce the course price 1000 CAD for all the courses. At the same time, we are going to transform all the course names to uppercase.\n",
    "dataframe2 =  dataframe.transform(to_upper_str_columns) \\\n",
    "        .transform(reduce_price,1000) \\\n",
    "        .transform(apply_discount)\n",
    "dataframe2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5930c9e1-af5b-4393-816d-163cf4aefd41",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "From the Exercise 1, we have got the following dataset\n",
    "\n",
    "rom our latest Movie dataset we got the following data:\n",
    "    \n",
    "data = [((1,4.2),(1,\"funny\")),\n",
    "((2,4.5),(3,\"funny\")),\n",
    "((1,4.0),(6,\"funny\")),\n",
    "((3,5.0),(47,\"action\")),\n",
    "((4,4.3),(50,\"romantic\")),\n",
    "((3,3.2),(70,\"biography\")),\n",
    "((4,5.0),(101,\"biography\")),\n",
    "((4,4.6),(110,\"Scientific\")),\n",
    "((1,5.0),(151,\"action\")),\n",
    "((1,4.6),( 157,\"action\")),\n",
    "((2,3.5),(167,\"funny\")),\n",
    "((1,4.1),(172,\"funny\")),\n",
    "((3,4.7),(181,\"action\")),\n",
    "((4,3.9),(192,\"romantic\")),\n",
    "((3,3.8),(201,\"biography\")),\n",
    "((4,5.0),(211,\"biography\")),\n",
    "((4,4.6),(224,\"Scientific\")),\n",
    "((1,5.0),(231,\"action\"))]\n",
    "\n",
    "the data is divided in the following structure: (userID, rating),(movieID, generes)\n",
    "\n",
    "The system got an issue and found that for some unaccountable reason, ratings for the \"Funny\" generes movies reduced by 15\\% and those reduced ratings were recorded. But this reduction didnot happen with all the ratings. It happened only for the ratings lower that 4.5.\n",
    "\n",
    "Please increase the ratings by 15\\% for the \"Funny\" Generes movies whose ratings are recorded lower than 4.5. List both the old ratings and new ratings side by side as shown in the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3439408-e382-4fa0-96ec-c91bc2c5f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Create SparkSession\n",
    "ss = SparkSession.builder.appName('Exercise 2').getOrCreate()\n",
    "\n",
    "# Data\n",
    "data = [((1,4.2),(1,\"funny\")),\n",
    "        ((2,4.5),(3,\"funny\")),\n",
    "        ((1,4.0),(6,\"funny\")),\n",
    "        ((3,5.0),(47,\"action\")),\n",
    "        ((4,4.3),(50,\"romantic\")),\n",
    "        ((3,3.2),(70,\"biography\")),\n",
    "        ((4,5.0),(101,\"biography\")),\n",
    "        ((4,4.6),(110,\"Scientific\")),\n",
    "        ((1,5.0),(151,\"action\")),\n",
    "        ((1,4.6),(157,\"action\")),\n",
    "        ((2,3.5),(167,\"funny\")),\n",
    "        ((1,4.1),(172,\"funny\")),\n",
    "        ((3,4.7),(181,\"action\")),\n",
    "        ((4,3.9),(192,\"romantic\")),\n",
    "        ((3,3.8),(201,\"biography\")),\n",
    "        ((4,5.0),(211,\"biography\")),\n",
    "        ((4,4.6),(224,\"Scientific\")),\n",
    "        ((1,5.0),(231,\"action\"))]\n",
    "\n",
    "# Schema\n",
    "schema = StructType([\n",
    "    StructField('user', StructType([\n",
    "        StructField('userID', IntegerType(), True),\n",
    "        StructField('rating', DoubleType(), True)\n",
    "    ])),\n",
    "    StructField('movie', StructType([\n",
    "        StructField('movieID', IntegerType(), True),\n",
    "        StructField('genre', StringType(), True)\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df = ss.createDataFrame(data, schema)\n",
    "\n",
    "# Adjust ratings for 'funny' genre where rating < 4.5\n",
    "df_adjusted = df.withColumn(\"old_rating\", col(\"user.rating\")) \\\n",
    "                .withColumn(\"new_rating\", \n",
    "                            when((col(\"movie.genre\") == \"funny\") & (col(\"user.rating\") < 4.5),\n",
    "                                 col(\"user.rating\") * 1.15)\n",
    "                            .otherwise(col(\"user.rating\")))\n",
    "\n",
    "# Display the result\n",
    "df_adjusted.select(\"user.userID\", \"movie.movieID\", \"movie.genre\", \"old_rating\", \"new_rating\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f683976d-5c00-4a51-95c8-8d08236470c1",
   "metadata": {},
   "source": [
    "## Union, Unionall and UnionByName\n",
    "\n",
    "PySpark union() and unionAll() transformations are used to merge two or more DataFrameâ€™s of the same schema or structure. The Union operation directly merge the two dataframes together one by one without seeing the data at all.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9d58882-149f-476f-85cf-30475042f0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000) \\\n",
    "  ]\n",
    "\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "dataframe = ss.createDataFrame(data = simpleData, schema = columns)\n",
    "dataframe.printSchema()\n",
    "dataframe.show(truncate=False)\n",
    "\n",
    "\n",
    "simpleData2 = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "columns2= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "\n",
    "dataframe2 = ss.createDataFrame(data = simpleData2, schema = columns2)\n",
    "\n",
    "dataframe2.printSchema()\n",
    "dataframe2.show(truncate=False)\n",
    "\n",
    "\n",
    "unionDF = dataframe.union(dataframe2)\n",
    "unionDF.printSchema()\n",
    "unionDF.show(truncate=False)\n",
    "\n",
    "\n",
    "unionAllDF = dataframe.unionAll(dataframe2)\n",
    "unionAllDF.printSchema()\n",
    "unionAllDF.show(truncate=False)\n",
    "\n",
    "unionAllDFbyName = dataframe.unionByName(dataframe2)\n",
    "unionAllDF.printSchema()\n",
    "unionAllDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e86093b-2af3-4717-ae45-32e949b56fa9",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Can you see the differences between Union, UnionAll and UnionByName? Please state them over here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6129ab-5303-4120-8a49-73e14153e069",
   "metadata": {},
   "source": [
    "<h1>Answer</h1>\n",
    "<p>union(): Combines two DataFrames and removes duplicates.</p>\n",
    "<p>unionAll(): Combines two DataFrames without removing duplicates.</p>\n",
    "<p>unionByName(): Combines DataFrames based on column names, not order.</p>\n",
    "<p>Use union() when you want to remove duplicates.\n",
    "Use unionAll() when you want to keep duplicates (or just use union() in modern PySpark).\n",
    "Use unionByName() when column orders are different, but the column names are the same.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250acc08-7642-498f-a09b-55b9e22d27da",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Consider the given data over here. Perform Union, UnionAll and UnionByName operation on these two given data.\n",
    "\n",
    "data1 = [((1,4.2),(1,\"funny\")),\n",
    "((2,4.5),(3,\"funny\")),\n",
    "((1,4.0),(6,\"funny\")),\n",
    "((3,5.0),(47,\"action\")),\n",
    "((4,4.3),(50,\"romantic\")),\n",
    "((3,3.2),(70,\"biography\")),\n",
    "((4,5.0),(101,\"biography\")),\n",
    "((4,4.6),(110,\"Scientific\")),\n",
    "((1,5.0),(151,\"action\")),\n",
    "((1,4.6),( 157,\"action\")),\n",
    "((2,3.5),(167,\"funny\")),\n",
    "((1,4.1),(172,\"funny\")),\n",
    "((3,4.7),(181,\"action\")),\n",
    "((4,3.9),(192,\"romantic\")),\n",
    "((3,3.8),(201,\"biography\")),\n",
    "((4,5.0),(211,\"biography\")),\n",
    "((4,4.6),(224,\"Scientific\")),\n",
    "((1,5.0),(231,\"action\"))]\n",
    "\n",
    "\n",
    "data2 = [((2,4.1),(1,\"funny\")),\n",
    "((1,4.2),(3,\"funny\")),\n",
    "((2,4.0),(6,\"funny\")),\n",
    "((1,4.0),(47,\"action\")),\n",
    "((2,4.7),(50,\"romantic\")),\n",
    "((1,3.6),(70,\"biography\")),\n",
    "((2,4.2),(101,\"biography\")),\n",
    "((4,4.7),(111,\"Scientific\")),\n",
    "((3,5.0),(151,\"action\")),\n",
    "((2,4.6),( 157,\"action\")),\n",
    "((1,3.5),(167,\"funny\")),\n",
    "((5,4.1),(172,\"funny\")),\n",
    "((2,4.5),(181,\"action\")),\n",
    "((3,4.3),(192,\"romantic\")),\n",
    "((2,4.2),(201,\"biography\")),\n",
    "((3,5.0),(211,\"biography\")),\n",
    "((3,4.6),(224,\"Scientific\")),\n",
    "((2,5.0),(231,\"action\"))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbddf0b0-6ee1-4d73-afec-7c6938b73bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union Result:\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o54.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 0.0 failed 1 times, most recent failure: Lost task 4.0 in stage 0.0 (TID 4) (172.17.18.224 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:108)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 40 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:108)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 40 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m union_df \u001b[38;5;241m=\u001b[39m df1\u001b[38;5;241m.\u001b[39munion(df2)\u001b[38;5;241m.\u001b[39mdistinct()\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnion Result:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m \u001b[43munion_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# UnionAll (includes duplicates, same as union without distinct)\u001b[39;00m\n\u001b[0;32m     44\u001b[0m union_all_df \u001b[38;5;241m=\u001b[39m df1\u001b[38;5;241m.\u001b[39munion(df2)\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:978\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    971\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    972\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    975\u001b[0m         },\n\u001b[0;32m    976\u001b[0m     )\n\u001b[1;32m--> 978\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o54.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 0.0 failed 1 times, most recent failure: Lost task 4.0 in stage 0.0 (TID 4) (172.17.18.224 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:108)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 40 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:108)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 40 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Union Example\").getOrCreate()\n",
    "\n",
    "# Data\n",
    "data1 = [((1,4.2),(1,\"funny\")), ((2,4.5),(3,\"funny\")), ((1,4.0),(6,\"funny\")), ((3,5.0),(47,\"action\")),\n",
    "         ((4,4.3),(50,\"romantic\")), ((3,3.2),(70,\"biography\")), ((4,5.0),(101,\"biography\")),\n",
    "         ((4,4.6),(110,\"Scientific\")), ((1,5.0),(151,\"action\")), ((1,4.6),(157,\"action\")),\n",
    "         ((2,3.5),(167,\"funny\")), ((1,4.1),(172,\"funny\")), ((3,4.7),(181,\"action\")),\n",
    "         ((4,3.9),(192,\"romantic\")), ((3,3.8),(201,\"biography\")), ((4,5.0),(211,\"biography\")),\n",
    "         ((4,4.6),(224,\"Scientific\")), ((1,5.0),(231,\"action\"))]\n",
    "\n",
    "data2 = [((2,4.1),(1,\"funny\")), ((1,4.2),(3,\"funny\")), ((2,4.0),(6,\"funny\")), ((1,4.0),(47,\"action\")),\n",
    "         ((2,4.7),(50,\"romantic\")), ((1,3.6),(70,\"biography\")), ((2,4.2),(101,\"biography\")),\n",
    "         ((4,4.7),(111,\"Scientific\")), ((3,5.0),(151,\"action\")), ((2,4.6),(157,\"action\")),\n",
    "         ((1,3.5),(167,\"funny\")), ((5,4.1),(172,\"funny\")), ((2,4.5),(181,\"action\")),\n",
    "         ((3,4.3),(192,\"romantic\")), ((2,4.2),(201,\"biography\")), ((3,5.0),(211,\"biography\")),\n",
    "         ((3,4.6),(224,\"Scientific\")), ((2,5.0),(231,\"action\"))]\n",
    "\n",
    "# Schema\n",
    "schema = StructType([\n",
    "    StructField(\"userInfo\", StructType([\n",
    "        StructField(\"userID\", IntegerType(), True),\n",
    "        StructField(\"rating\", DoubleType(), True)\n",
    "    ])),\n",
    "    StructField(\"movieInfo\", StructType([\n",
    "        StructField(\"movieID\", IntegerType(), True),\n",
    "        StructField(\"genre\", StringType(), True)\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Creating DataFrames\n",
    "df1 = spark.createDataFrame(data1, schema=schema)\n",
    "df2 = spark.createDataFrame(data2, schema=schema)\n",
    "\n",
    "# Union (removes duplicates by default)\n",
    "union_df = df1.union(df2).distinct()\n",
    "print(\"Union Result:\")\n",
    "union_df.show(truncate=False)\n",
    "\n",
    "# UnionAll (includes duplicates, same as union without distinct)\n",
    "union_all_df = df1.union(df2)\n",
    "print(\"UnionAll Result:\")\n",
    "union_all_df.show(truncate=False)\n",
    "\n",
    "# UnionByName (matches columns by name)\n",
    "union_by_name_df = df1.unionByName(df2)\n",
    "print(\"UnionByName Result:\")\n",
    "union_by_name_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d8fff-1838-4e21-b02f-48d4c5839a71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
