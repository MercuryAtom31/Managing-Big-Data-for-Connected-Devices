{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e208b897-d396-4bd6-89f6-28f6bde86d9a",
   "metadata": {},
   "source": [
    "# Managing Big Data for Connected Devices\n",
    "\n",
    "## 420-N63-NA\n",
    "\n",
    "## Kawser Wazed Nafi\n",
    " ---------------------------------------------------------------------------------------------------------------------------------Singular Value Decompositionression\n",
    "\n",
    "In this we'll make use of the California Housing data set.\n",
    "\n",
    "The data contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). In this sample a block group on average includes 1425.5 individuals living in a geographically compact area.\n",
    "\n",
    "These spatial data contain 20,640 observations on housing prices with 9 economic variables:\n",
    "\n",
    "#### Longitude\n",
    "refers to the angular distance of a geographic place north or south of the earth’s equator for each block group\n",
    "\n",
    "#### Latitude \n",
    "refers to the angular distance of a geographic place east or west of the earth’s equator for each block group\n",
    "\n",
    "#### Housing Median Age\n",
    "is the median age of the people that belong to a block group. Note that the median is the value that lies at the midpoint of a frequency distribution of observed values\n",
    "#### Total Rooms\n",
    "is the total number of rooms in the houses per block group\n",
    "\n",
    "#### Total Bedrooms\n",
    "is the total number of bedrooms in the houses per block group\n",
    "\n",
    "#### Population\n",
    "is the number of inhabitants of a block group\n",
    "\n",
    "#### Households\n",
    "refers to units of houses and their occupants per block group\n",
    "\n",
    "#### Median Income\n",
    "is used to register the median income of people that belong to a block group\n",
    "\n",
    "#### Median House Value\n",
    "is the dependent variable and refers to the median house value per block group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bbd6c27-be60-4513-b44a-536232537a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mllib in c:\\users\\hiche\\desktop\\spark\\venv\\lib\\site-packages (1.0.0a2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hiche\\desktop\\spark\\venv\\lib\\site-packages (from mllib) (75.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hiche\\desktop\\spark\\venv\\lib\\site-packages (from mllib) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hiche\\desktop\\spark\\venv\\lib\\site-packages (from requests->mllib) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hiche\\desktop\\spark\\venv\\lib\\site-packages (from requests->mllib) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hiche\\desktop\\spark\\venv\\lib\\site-packages (from requests->mllib) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hiche\\desktop\\spark\\venv\\lib\\site-packages (from requests->mllib) (2025.1.31)\n",
      "Requirement already satisfied: pandas in c:\\users\\hiche\\desktop\\spark\\venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\hiche\\desktop\\spark\\venv\\lib\\site-packages (from pandas) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hiche\\desktop\\spark\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hiche\\desktop\\spark\\venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hiche\\desktop\\spark\\venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hiche\\desktop\\spark\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hiche\\desktop\\spark\\venv\\lib\\site-packages (2.2.2)\n"
     ]
    }
   ],
   "source": [
    "# Install dependent libraries\n",
    "!pip install mllib\n",
    "!pip install pandas\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8187bf2f-4670-49a6-aec7-af68e5d012bd",
   "metadata": {},
   "source": [
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1996ac03-19c9-458b-94c7-d62ca165ebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a637803-68d6-4aeb-8e6b-e35dbf86821b",
   "metadata": {},
   "source": [
    "### Spark Session Creation with SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de38e604-faa9-4bd9-83bd-237b94ee146c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\pyspark\\sql\\context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+--------+--------+------+--------+------+--------+\n",
      "|   long|  lat|medage|totrooms|totbdrms|   pop|houshlds|medinc|   medhv|\n",
      "+-------+-----+------+--------+--------+------+--------+------+--------+\n",
      "|-122.23|37.88|  41.0|   880.0|   129.0| 322.0|   126.0|8.3252|452600.0|\n",
      "|-122.22|37.86|  21.0|  7099.0|  1106.0|2401.0|  1138.0|8.3014|358500.0|\n",
      "|-122.24|37.85|  52.0|  1467.0|   190.0| 496.0|   177.0|7.2574|352100.0|\n",
      "|-122.25|37.85|  52.0|  1274.0|   235.0| 558.0|   219.0|5.6431|341300.0|\n",
      "|-122.25|37.85|  52.0|  1627.0|   280.0| 565.0|   259.0|3.8462|342200.0|\n",
      "+-------+-----+------+--------+--------+------+--------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark Session Creation\n",
    "\n",
    "ss = SparkSession.builder.master(\"local[4]\").appName(\"Linear-Regression\").getOrCreate()\n",
    "sc = ss.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Load data from file into dataframe\n",
    "HOUSING_DATA = 'input/cal_housing.data'\n",
    "# define the schema, corresponding to a line in the csv data file.\n",
    "schema = StructType([\n",
    "    StructField(\"long\", FloatType(), nullable=True),\n",
    "    StructField(\"lat\", FloatType(), nullable=True),\n",
    "    StructField(\"medage\", FloatType(), nullable=True),\n",
    "    StructField(\"totrooms\", FloatType(), nullable=True),\n",
    "    StructField(\"totbdrms\", FloatType(), nullable=True),\n",
    "    StructField(\"pop\", FloatType(), nullable=True),\n",
    "    StructField(\"houshlds\", FloatType(), nullable=True),\n",
    "    StructField(\"medinc\", FloatType(), nullable=True),\n",
    "    StructField(\"medhv\", FloatType(), nullable=True)]\n",
    ")\n",
    "# Load housing data\n",
    "housing_df = ss.read.csv(path=HOUSING_DATA, schema=schema).cache()\n",
    "\n",
    "# Inspect first five rows\n",
    "housing_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271d1dc9-7449-4c47-a1ff-8288baf75b7d",
   "metadata": {},
   "source": [
    "### Loaded Data View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a53b47f-a3c0-4095-90b7-5d3698baa119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- long: float (nullable = true)\n",
      " |-- lat: float (nullable = true)\n",
      " |-- medage: float (nullable = true)\n",
      " |-- totrooms: float (nullable = true)\n",
      " |-- totbdrms: float (nullable = true)\n",
      " |-- pop: float (nullable = true)\n",
      " |-- houshlds: float (nullable = true)\n",
      " |-- medinc: float (nullable = true)\n",
      " |-- medhv: float (nullable = true)\n",
      "\n",
      "+------+--------+\n",
      "|   pop|totbdrms|\n",
      "+------+--------+\n",
      "| 322.0|   129.0|\n",
      "|2401.0|  1106.0|\n",
      "| 496.0|   190.0|\n",
      "| 558.0|   235.0|\n",
      "| 565.0|   280.0|\n",
      "| 413.0|   213.0|\n",
      "|1094.0|   489.0|\n",
      "|1157.0|   687.0|\n",
      "|1206.0|   665.0|\n",
      "|1551.0|   707.0|\n",
      "+------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------+-----+\n",
      "|medage|count|\n",
      "+------+-----+\n",
      "|  52.0| 1273|\n",
      "|  51.0|   48|\n",
      "|  50.0|  136|\n",
      "|  49.0|  134|\n",
      "|  48.0|  177|\n",
      "|  47.0|  198|\n",
      "|  46.0|  245|\n",
      "|  45.0|  294|\n",
      "|  44.0|  356|\n",
      "|  43.0|  353|\n",
      "+------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the dataframe columns\n",
    "housing_df.columns\n",
    "\n",
    "# show the schema of the dataframe\n",
    "housing_df.printSchema()\n",
    "\n",
    "# run a sample selection\n",
    "housing_df.select('pop','totbdrms').show(10)\n",
    "\n",
    "# group by housingmedianage and see the distribution\n",
    "result_df = housing_df.groupBy(\"medage\").count().sort(\"medage\", ascending=False)\n",
    "\n",
    "result_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb0b3ea-c181-4e65-804e-2da0af8bdcad",
   "metadata": {},
   "source": [
    "### Features Selection And Label Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25aec03e-c1df-4632-b4c0-b1c1e09ee2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target column\n",
    "feature_columns = [\"long\", \"lat\", \"medage\", \"totrooms\", \"totbdrms\", \"pop\", \"houshlds\", \"medinc\"]\n",
    "label_column = \"medhv\"\n",
    "\n",
    "features_df = housing_df.select(*feature_columns)\n",
    "label_df = housing_df.select(label_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891fe0fb-9f4b-4b6e-b53c-698c793cf4ab",
   "metadata": {},
   "source": [
    "### Spark Frame to to Pandas dataframe conversion\n",
    "\n",
    "As it is hard to locate data and extract them in Spark Dataframe, we temporarily need to convert the Spark Dataframe to Pandas dataframe. So, we are going to take the values loaded to RDD and work in local memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85ee5321-43e7-4c2c-bc96-6a17646ccaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame to Pandas DataFrame for SVD\n",
    "features_pd = features_df.toPandas()\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f764e638-ecd9-40e6-9ddd-6063c122f384",
   "metadata": {},
   "source": [
    "### Apply SVD for Dimensonality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "566d02f1-85cf-4752-950c-8fc798f1a230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SVD for dimensionality reduction\n",
    "n_components = 3  # Number of dimensions to reduce to\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "reduced_features = svd.fit_transform(features_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f899f99-c384-4b82-b075-4475c7ac41d9",
   "metadata": {},
   "source": [
    "### Convert Local Dataframe to Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c51293e-e7a1-4e2d-a18d-97624e63c18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the reduced features back to a Spark DataFrame\n",
    "reduced_columns = [f\"Component_{i+1}\" for i in range(n_components)]\n",
    "reduced_df = pd.DataFrame(reduced_features, columns=reduced_columns)\n",
    "reduced_spark_df = ss.createDataFrame(reduced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfee058-e9e0-4de2-ab3a-6a7fd339c5a3",
   "metadata": {},
   "source": [
    "### Add the label back to Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f528795-bcaf-4334-b1ee-5c552cc7db98",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o130.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 9.0 failed 1 times, most recent failure: Lost task 1.0 in stage 9.0 (TID 9) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m final_spark_df \u001b[38;5;241m=\u001b[39m reduced_spark_df\u001b[38;5;241m.\u001b[39mjoin(label_df, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Show the reduced dataset\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mfinal_spark_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    962\u001b[0m     )\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\hiche\\Desktop\\Spark\\venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o130.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 9.0 failed 1 times, most recent failure: Lost task 1.0 in stage 9.0 (TID 9) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n"
     ]
    }
   ],
   "source": [
    "# Add an index column to the reduced Spark DataFrame\n",
    "reduced_spark_df = reduced_spark_df.withColumn(\"index\", F.monotonically_increasing_id())\n",
    "\n",
    "# Add an index column to the label DataFrame\n",
    "label_df = label_df.withColumn(\"index\", F.monotonically_increasing_id())\n",
    "\n",
    "# Join the reduced features with the target column\n",
    "final_spark_df = reduced_spark_df.join(label_df, on=\"index\").drop(\"index\")\n",
    "\n",
    "# Show the reduced dataset\n",
    "final_spark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a68abb-e1fa-4600-a209-1f551bbf228e",
   "metadata": {},
   "source": [
    "Check the dataframe now. We started with 8 features column and reduced it to 3 columns only. This is call dimension reduction.\n",
    "\n",
    "In advanced data analytics, using heatmap people find out the features coefficient and decide which features are going to be combine to create the reduced dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3557ee-5c93-457e-a368-1e1c84227a22",
   "metadata": {},
   "source": [
    "### Save new dataframe to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80dc9a23-dbe1-40ab-8111-4bba9091f51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset saved to reduced_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the output path for the CSV file\n",
    "output_path = \"reduced_dataset.csv\"\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "final_spark_df.coalesce(1).write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "print(f\"Final dataset saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df973119-4d61-4a22-8383-5e0070b5dbe1",
   "metadata": {},
   "source": [
    "## Exercise 01\n",
    "Can you find out which features are merged under each component from the given california housing dataset? Please list them carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dab79f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Contribution per Principal Component:\n",
      "                 long       lat    medage  totrooms  totbdrms       pop  \\\n",
      "Component_1  0.075640 -0.073022 -0.218499  0.483771  0.490501  0.471968   \n",
      "Component_2 -0.701256  0.701977  0.016027  0.074611  0.060715  0.026036   \n",
      "Component_3 -0.055776  0.012536 -0.393864  0.093020 -0.117157 -0.116248   \n",
      "\n",
      "             houshlds    medinc  \n",
      "Component_1  0.491718  0.045144  \n",
      "Component_2  0.063521 -0.035300  \n",
      "Component_3 -0.109440  0.890917  \n",
      "\n",
      "Top 3 Features for Each Component:\n",
      "Component_1    [houshlds, totbdrms, totrooms]\n",
      "Component_2             [lat, long, totrooms]\n",
      "Component_3        [medinc, medage, totbdrms]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get feature names (original dataset features)\n",
    "feature_names = [\"long\", \"lat\", \"medage\", \"totrooms\", \"totbdrms\", \"pop\", \"houshlds\", \"medinc\"]\n",
    "\n",
    "# Create a DataFrame with SVD component loadings\n",
    "components_df = pd.DataFrame(svd.components_, columns=feature_names, index=[f\"Component_{i+1}\" for i in range(n_components)])\n",
    "\n",
    "# Display the component-feature matrix\n",
    "print(\"Feature Contribution per Principal Component:\")\n",
    "print(components_df)\n",
    "\n",
    "# Identify the top contributing features for each component\n",
    "top_features_per_component = components_df.apply(lambda x: x.abs().nlargest(3).index.tolist(), axis=1)\n",
    "print(\"\\nTop 3 Features for Each Component:\")\n",
    "print(top_features_per_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f07f53",
   "metadata": {},
   "source": [
    "## Exercise 02\n",
    "Can you list 4 points on why do we need to reduce the dataset, whenever required? Please explain the points properly with example, if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3694b5c",
   "metadata": {},
   "source": [
    "### ANSWER:\n",
    "\n",
    "Reducing the dataset (e.g., using dimensionality reduction techniques like SVD or PCA) is crucial for better performance and interpretability.<br> \n",
    "\n",
    "#### Here are four main reasons:\n",
    "\n",
    "1. Improves Computational Efficiency<br>\n",
    "Reason: Large datasets require more memory and processing power, leading to slower computations.<br>\n",
    "Example: In machine learning, training a model on 1 million features would be much slower than training on a reduced subset of 100 features.<br>\n",
    "2. Removes Redundant and Irrelevant Features<br>\n",
    "Reason: Some features may be highly correlated or provide little additional information.<br>\n",
    "Example: In a real estate dataset, total_rooms and total_bedrooms are strongly correlated, so keeping both is unnecessary.<br>\n",
    "3. Reduces Overfitting and Improves Model Generalization<br>\n",
    "Reason: Too many features can cause overfitting, where a model learns noise instead of patterns.<br>\n",
    "Example: In a spam detection model, if we use thousands of email metadata features, the model might learn dataset-specific noise rather than general spam indicators.<br>\n",
    "4. Enhances Interpretability<br>\n",
    "Reason: A smaller set of features makes it easier to analyze and explain results.<br>\n",
    "Example: In finance, reducing hundreds of stock market indicators to 3 key components (e.g., \"market trend\", \"volatility\", \"sector growth\") makes insights more actionable.<br>\n",
    "Conclusion<br>\n",
    "Reducing the dataset boosts performance, removes unnecessary noise, prevents overfitting, and makes results easier to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9980c9",
   "metadata": {},
   "source": [
    "## Exercise 03\n",
    "You are working with insurance.csv file on which after encoding the string values you are supposed to get a list of features (more than 10). It is now a dataset with big dimension (more than 10). Can you please reduce the dimension of the dataset to 4 columns/features? Please print the state of before dimension reduction and after dimension reduction state of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d911bb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Dimension Reduction: (1338, 9)\n",
      "After Dimension Reduction: (1338, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Component_1</th>\n",
       "      <th>Component_2</th>\n",
       "      <th>Component_3</th>\n",
       "      <th>Component_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.506193</td>\n",
       "      <td>-0.868697</td>\n",
       "      <td>1.712276</td>\n",
       "      <td>-2.086279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.449763</td>\n",
       "      <td>2.097010</td>\n",
       "      <td>-0.796116</td>\n",
       "      <td>-1.252142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.088781</td>\n",
       "      <td>1.755718</td>\n",
       "      <td>-0.625870</td>\n",
       "      <td>-0.164003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.594631</td>\n",
       "      <td>-1.672266</td>\n",
       "      <td>-1.292505</td>\n",
       "      <td>-0.760977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.317052</td>\n",
       "      <td>-0.880339</td>\n",
       "      <td>-1.336580</td>\n",
       "      <td>-0.505571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Component_1  Component_2  Component_3  Component_4\n",
       "0     0.506193    -0.868697     1.712276    -2.086279\n",
       "1    -0.449763     2.097010    -0.796116    -1.252142\n",
       "2    -0.088781     1.755718    -0.625870    -0.164003\n",
       "3    -0.594631    -1.672266    -1.292505    -0.760977\n",
       "4    -1.317052    -0.880339    -1.336580    -0.505571"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"insurance.csv\")\n",
    "\n",
    "# Encode categorical variables (if any)\n",
    "df_encoded = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Standardize the dataset\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_encoded)\n",
    "\n",
    "# Print shape before dimensionality reduction\n",
    "print(f\"Before Dimension Reduction: {df_scaled.shape}\")  # (rows, features)\n",
    "\n",
    "# Apply SVD to reduce to 4 features\n",
    "n_components = 4\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "df_reduced = svd.fit_transform(df_scaled)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "df_reduced = pd.DataFrame(df_reduced, columns=[f\"Component_{i+1}\" for i in range(n_components)])\n",
    "\n",
    "# Print shape after dimensionality reduction\n",
    "print(f\"After Dimension Reduction: {df_reduced.shape}\")  # (rows, 4)\n",
    "\n",
    "# Display first 5 rows\n",
    "df_reduced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c054b6c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
