{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb5278b-1d80-4a0c-a0c1-f6f6dd1e2d97",
   "metadata": {},
   "source": [
    "# WordMap Counting Problem All Together\n",
    "\n",
    "We have done a lot to make our understanding better with MapReduce framework in PySpark.\n",
    "\n",
    "Now, we are going to compose all of them together .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aad7bb-fe07-436d-987a-4bdd37a6117b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf7df7c-f87b-4760-bc4b-3c0b46d61753",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "\n",
    "ss = SparkSession.builder.master(\"local[4]\").appName(\"FlatMap-ReduceByKey\").getOrCreate();\n",
    "sc = ss.sparkContext\n",
    "\n",
    "\n",
    "\n",
    "lines = [\n",
    "    \"word count from Wikipedia the free encyclopedia\",\n",
    "    \"the word count is the number of words in a document or passage of text Word counting may be needed when a text\",\n",
    "    \"is required to stay within certain numbers of words This may particularly be the case in academia legal\",\n",
    "    \"proceedings journalism and advertising Word count is commonly used by translators to determine the price for\"\n",
    "]\n",
    "\n",
    "# Create Rdd using parallelize\n",
    "rdd = sc.parallelize(lines)\n",
    "rddFlatMap = rdd.flatMap(lambda line:line.split(\" \"))\n",
    "rddMap = rddFlatMap.map(lambda word: (word, 1))\n",
    "rddReduced = rddMap.reduceByKey(lambda x, y : x + y)\n",
    "rddReduced.collect()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f8aa4e-a1c8-40d4-b591-35898823361e",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Take the input from data.txt file and count the number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c2eba7b-e5f6-4da7-bb40-cfb031260614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: Alice’s, Count: 18\n",
      "Word: by, Count: 18\n",
      "Word: Lewis, Count: 18\n",
      "Word: Carroll, Count: 18\n",
      "Word: eBook, Count: 27\n",
      "Word: for, Count: 27\n",
      "Word: use, Count: 27\n",
      "Word: of, Count: 27\n",
      "Word: anyone, Count: 27\n",
      "Word: at, Count: 27\n",
      "Word: no, Count: 27\n",
      "Word: cost, Count: 27\n",
      "Word: and, Count: 27\n",
      "Word: with, Count: 27\n",
      "Word: Project, Count: 9\n",
      "Word: Gutenberg’s, Count: 9\n",
      "Word: Adventures, Count: 18\n",
      "Word: in, Count: 18\n",
      "Word: Wonderland, Count: 18\n",
      "Word: This, Count: 27\n",
      "Word: is, Count: 27\n",
      "Word: the, Count: 27\n",
      "Word: anywhere, Count: 27\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a new Spark session\n",
    "ss = SparkSession.builder.master(\"local[4]\").appName(\"WordCount\").getOrCreate()\n",
    "sc = ss.sparkContext\n",
    "\n",
    "# Read the content of data.txt file\n",
    "file_path = \"textData.txt\" \n",
    "rdd = sc.textFile(file_path)\n",
    "\n",
    "# Use flatMap to split the lines into words\n",
    "rddFlatMap = rdd.flatMap(lambda line: line.split())\n",
    "\n",
    "# Map each word to a tuple (word, 1)\n",
    "rddMap = rddFlatMap.map(lambda word: (word, 1))\n",
    "\n",
    "# Use reduceByKey to count the occurrences of each word\n",
    "rddReduced = rddMap.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Collect the result and print it\n",
    "result = rddReduced.collect()\n",
    "\n",
    "for word, count in result:\n",
    "    print(f\"Word: {word}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cec6ce3-f131-4f40-94d7-08a8b63a18ec",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Load the testData.txt in the rdd and add all the values together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0123ed1-f39f-4b7e-9d0b-0fdf33ef7911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum of Values: 680\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a new Spark session\n",
    "ss = SparkSession.builder.master(\"local[4]\").appName(\"SumValues\").getOrCreate()\n",
    "sc = ss.sparkContext\n",
    "\n",
    "# Read the content of testData.txt file\n",
    "file_path = \"testDataM.txt\"  # You can specify the path to the file here\n",
    "rdd = sc.textFile(file_path)\n",
    "\n",
    "# Convert the string values into integers\n",
    "rdd_int = rdd.flatMap(lambda line: line.split()).map(lambda x: int(x))\n",
    "\n",
    "# Sum all the values\n",
    "total_sum = rdd_int.reduce(lambda x, y: x + y)\n",
    "\n",
    "print(f\"Total Sum of Values: {total_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d6c181-44ea-4d34-a049-c40d6908824d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
